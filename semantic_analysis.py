# -*- coding: utf-8 -*-
"""semantic-analysis.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1okmoTm4ffAca-LUjt9X-2tyVP7-zxQOL

# **Preprocessing Code start**
"""

import pandas as pd
data = pd.read_csv('/content/eng.csv')
print(data.head())

#expanding the dispay
pd.set_option('display.max_colwidth', None)
data= data [['text']]
data.head()

#checking -> text column er kon line koi bar ache
data['text'].value_counts()

#library that contains punctuation
import string
string.punctuation

#defining the function to remove punctuation
def remove_punctuation(text):
    punctuationfree="".join([i for i in text if i not in string.punctuation])
    return punctuationfree
#storing the puntuation free text
data['clean_text']= data['text'].apply(lambda x:remove_punctuation(x))
data.head()

data['lower_text']= data['clean_text'].apply(lambda x: x.lower())
data.head()

#defining function for tokenization(Sentences are tokenized into words.)
import re
def tokenization(text):
    tokens = re.split('W+',text)
    return tokens
#applying function to the column
data['tokenied_text']= data['lower_text'].apply(lambda x: tokenization(x))
data.head()

#Stop words present in the library
import nltk
stopwords = nltk.corpus.stopwords.words('english')
stopwords[0:10]
['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', "you're"]
#defining the function to remove stopwords from tokenized text
def remove_stopwords(text):
    output= [i for i in text if i not in stopwords]
    return output
#applying the function
data['no_stopwords']= data['tokenied_text'].apply(lambda x:remove_stopwords(x))
data.head()

#importing the Stemming function from nltk library
from nltk.stem.porter import PorterStemmer
#defining the object for stemming
porter_stemmer = PorterStemmer()

#defining a function for stemming
def stemming(text):
  stem_text = [porter_stemmer.stem(word) for word in text]
  return stem_text
data['stemmed_text']=data['no_stopwords'].apply(lambda x: stemming(x))
data.head()

from nltk.stem import WordNetLemmatizer
#defining the object for Lemmatization
wordnet_lemmatizer = WordNetLemmatizer()
#defining the function for lemmatization
def lemmatizer(text):
  lemm_text = [wordnet_lemmatizer.lemmatize(word) for word in text]
  return lemm_text
data['lemmatized_text']=data['no_stopwords'].apply(lambda x:lemmatizer(x))
data.head()

"""## **Preprocessing Code Done**

# BoW Code Starts(text to numeric feature)
"""

from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer

# ... (Your code for loading and preprocessing, ending with the 'lemmatized_text' column) ...

# Create BoW representation using CountVectorizer with N-grams (e.g., 1 to 3)
vectorizer = CountVectorizer(ngram_range=(1, 3),
                             token_pattern=r'\b[a-zA-Z]+\b',  # Keep only alphabetic tokens
                             stop_words='english')  # Use built-in English stop words
bow_matrix = vectorizer.fit_transform([' '.join(text) for text in data['lemmatized_text']])

tfidf_transformer = TfidfTransformer()
tfidf_matrix = tfidf_transformer.fit_transform(bow_matrix)


vocabulary = vectorizer.get_feature_names_out()
print("Vocabulary (Wordset):\n", vocabulary)

bow_df = pd.DataFrame(bow_matrix.toarray(), columns=vocabulary)
print("\nBoW DataFrame (Output):\n", bow_df.head())

document_index = 0  # Index of the document you want to see
bow_vector = bow_df.iloc[document_index]
print("\nBoW Vector for Document", document_index, ":\n", bow_vector)

"""# BoW Code done

**this portion of code is only for eda**
"""

# this portion of code is only for eda
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
lemmatized_text = data['lemmatized_text']


# Calculate class distribution
data = pd.read_csv('/content/eng.csv')
emotion_columns = data[['Anger', 'Fear', 'Joy', 'Sadness', 'Surprise']].sum()
data['lemmatized_text'] = lemmatized_text

# Plot class distribution
plt.figure(figsize=(8, 5))
sns.barplot(x=emotion_columns.index, y=emotion_columns.values)
plt.title('Distribution of Emotions')
plt.xlabel('Emotions')
plt.ylabel('Frequency')
plt.show()

# 2. Text Length and Word Count Analysis

# Calculate text length and word count
data['text_length'] = data['text'].apply(len)
data['word_count'] = data['text'].apply(lambda x: len(x.split()))

# Plot histograms
plt.figure(figsize=(12, 5))
plt.subplot(1, 2, 1)
sns.histplot(data['text_length'], bins=20)
plt.title('Distribution of Text Length')
plt.xlabel('Text Length (characters)')

plt.subplot(1, 2, 2)
sns.histplot(data['word_count'], bins=20)
plt.title('Distribution of Word Count')
plt.xlabel('Word Count')

plt.tight_layout()
plt.show()


# 3. Most Frequent Words (Before and After Preprocessing)

# Before preprocessing
from collections import Counter

all_words = ' '.join(data['text']).lower().split()
word_freq = Counter(all_words)
top_words = word_freq.most_common(20)  # Get top 20 frequent words

plt.figure(figsize=(10, 6))
sns.barplot(x=[word[0] for word in top_words], y=[word[1] for word in top_words])
plt.title('Top 20 Most Frequent Words (Before Preprocessing)')
plt.xlabel('Words')
plt.ylabel('Frequency')
plt.xticks(rotation=45, ha='right')
plt.show()

# After preprocessing (using 'lemmatized_text' column)
all_lemmatized_words = [word for sublist in data['lemmatized_text'] for word in sublist]
lemmatized_word_freq = Counter(all_lemmatized_words)
top_lemmatized_words = lemmatized_word_freq.most_common(20)

plt.figure(figsize=(10, 6))
sns.barplot(x=[word[0] for word in top_lemmatized_words], y=[word[1] for word in top_lemmatized_words])
plt.title('Top 20 Most Frequent Words (After Preprocessing)')
plt.xlabel('Words')
plt.ylabel('Frequency')
plt.xticks(rotation=45, ha='right')
plt.show()

